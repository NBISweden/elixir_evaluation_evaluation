# Introduction

On 2025-01-20 the NBIS Training Steering Group had
a meeting on course evaluations.
The first question was 'How are evaluations evaluated?'.

It is common practice that courses are evaluated by surveys
`[Brazas & Ouellette, 2016][Gurwitz et al., 2020][Jordan et al., 2018]`.

This does not mean that the questions in such surveys are useful.
For example, 2 out of 3 teachers of one NBIS course have the shared verdict
that the NBIS questions are useless, where 1 is neutral and
reasoning 'it is what we commonly do'.

???- question "Source?"

    You can find one teacher's written-down criticism
    [here](https://uppmax.github.io/programming_formalisms/misc/evaluation/).
    He openly uses the term 'useless'.

    You can find the meeting notes where it was decided unanymously
    against using the survey in unmodified form
    [here](https://uppmax.github.io/programming_formalisms/meeting_notes/20241010/).
    In this meeting, only orally, the verdict on this survey was:

    - 2x: useless
    - 1x: neutral, as '[such surveys] is what we commonly do'

This disagreement is used as a starting point in evaluating the
NBIS course evaluation questions,
where the literature is searched for how these questions
came to be and by which criteria the best were selected,
in the hope of establishing the usefulness of this survey.

???- question "Which alternatives are suggested?"

    One alternative that is suggested is to use learning outcomes:

    - [An example evaluation using learning outcomes](https://uppmax.github.io/bianca_workshops/evaluations/20241111/)

    Another alternative that is suggested is to ask for written feedback
    only to rate teachers:

    - [An example reflection that used written feedback for its teachers](https://uppmax.github.io/programming_formalisms/reflections/2024_autumn/20241122_richel/)

    Also, there is a clear rule to assess the usefulness of an evaluation
    questions: 'An evaluation question is useful if it is used in a reflection'.
    Reflections that apply these rules:

    - [Example 1, where some questions are assessed to be useless](https://github.com/UPPMAX/R-python-julia-matlab-HPC/blob/main/reflections/20241024_richel/README.md#evaluation-results)
    - [Example 2, where all questions are used](https://uppmax.github.io/programming_formalisms/reflections/2024_autumn/20241122_richel/)

Besides discussing the current survey question,
this paper is the first to give a fully transparent process
on how, with the same goals in mind, a similar set of
evaluation questions were developed
and how the best questions of this set were selected.

## References

- `[Brazas & Ouellette, 2016]`
  Brazas, Michelle D., and BF Francis Ouellette.
  "Continuing education workshops in bioinformatics positively impact
 research and careers." PLoS computational biology 12.6 (2016): e1004916.
- `[Gurwitz et al., 2020]`
  Gurwitz, Kim T., et al.
  "A framework to assess the quality and impact of bioinformatics training
  across ELIXIR." PLoS computational biology 16.7 (2020): e1007976.
  [website](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007976)
- `[Jordan et al., 2018]`
  Jordan, Kari, François Michonneau, and Belinda Weaver.
  "Analysis of Software and Data Carpentry’s pre-and post-workshop surveys."
  Software Carpentry. Retrieved April 13 (2018): 2023.
  [PDF](papers/jordan_et_al_2018.pdf)
