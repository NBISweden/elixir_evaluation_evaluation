{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Improving the ELIXIR evaluation for both management and trainers","text":"<p>An evaluation of the ELIXIR course evaluation.</p> <ul> <li>Download the PDF of the draft paper</li> </ul>"},{"location":"#goals","title":"Goals","text":"<ul> <li>To evaluate the mandatory ELIXIR course evaluation questions   regarding course quality</li> </ul> What are these ELIXIR evaluation questions? <p>Read the ELIXIR evaluation questions.</p> <p>This paper deals only with the mandatory questions 5 to and including 9.</p> <ul> <li>If possible, propose a better set of questions</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>This evaluation is written as a scientific paper, split over multiple pages (for now):</p> <ul> <li>Authors: Rich\u00e8l Bilderbeek, Daniel Wibberg</li> <li>Abstract</li> <li>Introduction</li> <li>Research questions</li> <li>Methods:<ul> <li>RQ1 Methods</li> <li>RQ2 Methods</li> <li>RQ3 Methods</li> <li>RQ4 Methods</li> </ul> </li> <li>Results:<ul> <li>RQ1 Results</li> <li>RQ2 Results</li> <li>RQ3 Results</li> <li>RQ4 Results</li> </ul> </li> <li>Conclusion</li> <li>Discussion</li> <li>Appendix<ul> <li>The current ELIXIR short-term evaluation</li> </ul> </li> </ul>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at marcus.lundberg@uppmax.uu.se. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Thanks for considering to contribute and reading this!</p> <p>Here we discuss how to contribute</p> <ul> <li>Spoken text, e.g. ideas, feedback, messages, etc.   and are written in English.</li> <li>Code, e.g. textual changes where the text is formatted in Markdown</li> </ul>"},{"location":"CONTRIBUTING/#spoken-text","title":"Spoken text","text":"<p>Spoken text are ideas, feedback, messages, etc. and are written in English.</p> <p>For ideas or feedback, create an Issue. These Issues will be discussed in a meeting and/or below that Issue.</p> <p>Ideas that improve the experience of our learners will likely be accepted.</p>"},{"location":"CONTRIBUTING/#code","title":"Code","text":"<p>We welcome any contribution that:</p> <ul> <li>improves the experience of our learners</li> <li>follow all standards set by the continuous integration tools   (e.g. use 4 spaces for indentation)</li> </ul> <p>As an UPPMAX contributor, one can contribute by:</p> <ol> <li>Clone this repository</li> <li>Add a branch</li> <li>Work on your branch</li> <li>When done, create a Pull Request from your branch to <code>main</code></li> <li>If the change is accepted after review it will be merged into the main branch</li> <li>Your branch will be deleted after merging</li> </ol> <p>As an external contributor, one can contribute by:</p> <ol> <li>Fork this repository</li> <li>Modify your Fork</li> <li>When done, creating a Pull Request from your Fork to this repository,    merging to the <code>main</code> branch is fine :-)</li> <li>If the change is accepted after review it will be merged into the main branch</li> </ol>"},{"location":"abstract/","title":"Abstract","text":"<p>NBIS teaches, among others, bioinformatics courses, which are evaluated with an anonymous survey sent its learners. Part of this survey consists of mandatory questions to assess course quality. However, there is discussion in how useful these questions are in achieving their goal. Here, we describe the history of this survey, how its questions were crafted and selected, followed by an evaluation of its final form. We find that no selection criterium has been written down and evidence-based best practices by the academic literature were ignored. This paper is the first to transparently show the crafting and selection of evaluation questions to be used for NBIS evaluation, being the 'future work' as mentioned in its paper most important to this topic.</p>"},{"location":"conclusion/","title":"5. Conclusion","text":""},{"location":"conclusion/#51-rq1-what-is-the-history-of-the-elixir-evaluation-questions","title":"5.1. RQ1: What is the history of the ELIXIR evaluation questions?","text":"<p>The ELIXIR student evaluation is based on two evaluations. Neither evaluation describes (1) how its questions were developed, (2) by which criteria the best questions were selected.</p> <p>From these two questionnaires, some (but not all) questions were selected to be put in the ELIXIR evaluation. Additionally, ELIXIR added new questions, regarding student satisfactions. The criteria for neither not copying questions for the earlier questionnaires, not for adding adding questions were not written down.</p> <p>As none of these evaluations describe the process on how questions were developed, it comes as no surprise that neither evaluation refers to evidence-based best practices in the literature.</p>"},{"location":"conclusion/#52-rq2-how-does-the-academic-literature-relate-to-the-elixir-evaluation-questions","title":"5.2. RQ2: How does the academic literature relate to the ELIXIR evaluation questions?","text":"<p>The mandatory ELIXIR evaluation questions with the goal of evaluating course quality have the following relation to the academic literature</p> Question Relation to the literature 5 None found 6 None found 7 One paper which states that this question is suitable to assess course quality 8 Many papers, including a meta-analysis that this question is unsuitable to assess course quality 9 None found"},{"location":"conclusion/#53-rq3-which-elixir-evaluation-questions-are-concluded-from-a-fully-transparent-process","title":"5.3. RQ3: Which ELIXIR evaluation questions are concluded from a fully transparent process?","text":"<p><code>TODO</code></p>"},{"location":"conclusion/#54-rq4-how-different-are-the-newly-suggested-questions-from-the-current-ones","title":"5.4. RQ4: How different are the newly suggested questions from the current ones?","text":"<p><code>TODO</code></p>"},{"location":"data_set_1_merge/","title":"Data set 1 merge","text":"<p>There were 5 questions.</p> <p>Of those 5, here are 2 questions with different spelling:</p> <pre><code>What is your favorite color?,Open question\nWhat is your favorite colour?,Open question\n</code></pre> <p>RJCB removed the one with <code>colour</code>, resulting in 4 questions.</p> <p>Of the remaining 4, here are 2 identical questions:</p> <pre><code>What is your favorite animal?,Open question\nWhat is your favorite animal?,Open question\n</code></pre> <p>RJCB removed one of these duplicates, resulting in 3 questions.</p>"},{"location":"discussion/","title":"6. Discussion","text":"<p>First and foremost, one can argue that a questionnaire is simply a questionnaire. That the development of a questionnaire is 'just' a group effort. That the people in such groups are constrained by time. And that this is a good enough excuse. However, this overlooks the fact that at least sixteen thousand participants have taken the time to fill in this questionnaire. If we care about our participants, maybe we should care about the usefulness of a questionnaire we send to each of them.</p>"},{"location":"discussion/#61-rq1-what-is-the-history-of-the-elixir-evaluation-questions","title":"6.1. RQ1: What is the history of the ELIXIR evaluation questions?","text":"<p>Although this is not formally published, maybe the current survey (or the surveys it is based on) have been developed by evidence-based best practices, yet in unpublished and more informal documents. However, it seems unlikely that references to the literature are used in informal communication, yet subsequently omitted when a formal academic paper is written.</p> <p>To us, it seems more likely that a too short amount of time was allocated to researching the academic literature, resulting in missing the papers mentioned in this paper.</p>"},{"location":"discussion/#62-rq2-how-does-the-academic-literature-relate-to-the-elixir-evaluation-questions","title":"6.2. RQ2: How does the academic literature relate to the ELIXIR evaluation questions?","text":"<p>The majority of the ELIXIR mandatory evaluation questions, in the section to assess course quality have little connection to the academic literature. Three out of five questions (i.e. questions 5, 6 and 9) resulted in zero papers being written on their effectiveness.</p> <p>To us, these questions simply seem to be in the wrong session. Why it was chosen to put these questions in the section called 'quality metrics', instead of a (new) section with a better fitting name is unknown.</p> <p>Regarding the two questions that seem to be in the correct section, however, only one is supported by the literature ('Would you recommend the course?') by one paper, where the other ('How satisfied are you with the course?') has strong support against its effectiveness.</p> <p>Regarding these two questions, they seem to be measuring the same thing: course satisfaction (on its own), and recommending a course (because the learner is satisfied with the course). It is unknown why these two similar questions are both in the evaluation and it would be interesting to see how strong the correlation is between the answers on these two questions.</p>"},{"location":"discussion/#63-epilogue","title":"6.3. Epilogue","text":"<p>We know that a teacher reflecting on his/her work is one of the best ways to increase his/her teaching quality. Or: 'student ratings can only become a tool for enhancement when they feed reflective conversations about improving the learning process and when these conversations are informed by the scholarship of teaching and learning <code>[Rox\u00e5 et al., 2021]</code>. The other best way for teachers to improve is to do peer observations. Note that neither practice needs an evaluations.</p> <p>If we really care about teaching quality, shouldn't we encourage doing the things that actually work?</p>"},{"location":"discussion/#64-references","title":"6.4. References","text":"<ul> <li><code>[Rox\u00e5 et al., 2021]</code> Rox\u00e5, Torgny, et al.   \"Reconceptualizing student ratings of teaching to support quality discourse   on student learning: a systems perspective.\" Higher Education (2021): 1-21.</li> </ul>"},{"location":"elixir_evaluation/","title":"A1. NBIS Short Term Feedback (STF)","text":"<pre><code>Besides this text, this page shows the current\nNBIS Short Term Feedback form. Except for changes in layout\nand the numbering of sections, \nthe content is unmodified.\n</code></pre>"},{"location":"elixir_evaluation/#a11core-question-set-information","title":"A1.1.Core question set information","text":"<p>The intention of the STF survey is to find out how participants have used the skills and knowledge they gained through participating in the NBIS course.</p> <p>The STF survey aims to provide data back to NBIS from course participants.</p> <p>The survey should preferably be given by the course leader to the participants on the last day of the course. Some of the questions below are CORE Questions and needs to always be included in the survey. There are also room for ADDITIONAL questions that can be modified for respective course.</p> <ul> <li>Contents</li> <li>Important Information</li> <li>Core Question Set</li> <li>Demographic Information</li> <li>Quality Metrics</li> <li>Additional Questions - Training content/information</li> <li>Additional Questions - Training logistics</li> </ul>"},{"location":"elixir_evaluation/#a12-important-information","title":"A1.2. Important Information","text":"<p>Below are the core questions for NBIS short term feedback (STF), which are required to be captured for all NBIS training events from August 2018 onwards, most typically in an end-of-training-event feedback survey (i.e. exit survey). The information and Core questions are extracted from the ELIXIR and ELIXIR-EXCELERATE courses. Additional questions are free to be modified to suit the course needs. The format for collecting the data is up to each training provider, although results should be exportable to Excel format. The core questions may be divided into two categories and will by and large be analysed separately - both categories are required to be captured:</p> <ul> <li>Demographic information</li> <li>Quality metrics</li> </ul> <p>For the demographic information questions specifically, these may be captured either in the exit survey OR in the registration form. The exit survey should be administered as close as possible to the end of the training event, preferably on the last day of the course. Please add the result of the survey to the course folder in Google Drive (NBIS Course Catalogue).</p> <p>The core question set is followed by a set of Additional (suggested) questions that training organisers might also like to ask. Please note: while the core question set is compulsory, Course leader(s) are encouraged to ask any additional questions for their own collection and data analysis, should they wish.</p> <p>Data formatting: Preferred column headers for each core metric are in \u2018red\u2019. It would be very helpful for analysing the data if everyone used these column headings when exporting the results. Please note: these descriptors are case sensitive (e.g. use <code>advertised</code> not <code>Advertised</code>). Also, the underscores are important! (e.g. <code>career_stage</code> is NOT the same as <code>career stage</code>).</p> <p>If possible, please name the dataset file as follows to assist with data handling: <code>YYYY-MM-DD_L/STF_Location_CourseName</code>, e.g. <code>2018-06-11_STF_Visby_RaukR</code></p>"},{"location":"elixir_evaluation/#a13-core-question-set","title":"A1.3. Core Question Set","text":""},{"location":"elixir_evaluation/#a14-section-1-template-nbis-short-term-feedback-stf-survey-course-name-location-yyyy-mm-dd","title":"A1.4. Section 1 - Template: NBIS Short Term Feedback (STF) survey <code>COURSE NAME, LOCATION, YYYY-MM-DD</code>","text":"<p>Thank you for filling the questionnaire. It is really important to us in order to continually improve the course and the materials we deliver. In filling the questionnaire, please keep in mind that your comments - which are not mandatory - are especially precious. We may share anonymised information with course presenters and developers as well as for wider quality/impact analyses.</p>"},{"location":"elixir_evaluation/#a15-section-2-demographic-information","title":"A1.5. Section 2 - Demographic Information","text":"<p><code>1</code>. Where did you see the course advertised? <code>advertised</code></p> <ul> <li><code>a</code>. NBIS website</li> <li><code>b</code>. SciLifeLab website</li> <li><code>c</code>. Social Media (e.g. NBIS twitter)</li> <li><code>d</code>. Host Institute website</li> <li><code>e</code>. Colleague</li> <li><code>f</code>. TeSS</li> <li><code>g</code>. Email</li> <li><code>h</code>. Internet search</li> <li><code>i</code>. Other (comments)</li> </ul> <p><code>2</code>. What is your career stage? <code>career_stage</code></p> <ul> <li><code>a</code>. PhD candidate</li> <li><code>b</code>. Postdoctoral researcher</li> <li><code>c</code>. Senior researcher/Principal investigator</li> <li><code>d</code>. Staff scientist</li> <li><code>e</code>. Industry scientist</li> <li><code>f</code>. Other (comments)</li> </ul> <p><code>3</code>. What is your host university? <code>host_university</code></p> <p><code>4</code>. Gender <code>gender</code></p> <ul> <li><code>a</code>. Male</li> <li><code>b</code>. Female</li> <li><code>c</code>. Prefer not to say</li> <li><code>d</code>. Other (please specify)</li> </ul>"},{"location":"elixir_evaluation/#a16-section-3-quality-metrics","title":"A1.6. Section 3 - Quality Metrics","text":"<p><code>5</code>. Have you used the tools/resource(s) covered in the course before? <code>have_used_resources_before</code></p> <ul> <li><code>1</code>. Never - Unaware of them</li> <li><code>2</code>. Never - Used other service</li> <li><code>3</code>. Occasionally</li> <li><code>4</code>. Frequently</li> </ul> <p><code>6</code>. Will you use the tools/resource(s) covered in the course again? <code>will_use_resources_future</code></p> <ul> <li><code>1</code>. Yes</li> <li><code>2</code>. No</li> <li><code>3</code>. Maybe</li> </ul> <p><code>7</code>. Would you recommend the course? <code>would_recommend_course</code></p> <ul> <li><code>1</code>. Yes</li> <li><code>2</code>. No</li> <li><code>3</code>. Maybe</li> </ul> <p><code>8</code>. What is your overall rating for the course*. <code>overall_satisfaction</code></p> <ul> <li><code>a</code>. Poor (1)</li> <li><code>b</code>. Satisfactory (2)</li> <li><code>c</code>. Good (3)</li> <li><code>d</code>. Very Good (4)</li> <li><code>e</code>. Excellent (5)</li> </ul> <p>(*please include both numeric and categorical scale for this question.)</p> <p><code>9</code>. A. May we contact you by email in the future for more feedback? <code>contact_future</code></p> <ul> <li><code>1</code>. Yes</li> <li><code>2</code>. No</li> </ul> <p>9 B. If you answered \u2018yes\u2019 to the above question, please enter your email address, below. email ( Information for question 9B must be collected and stored by each Node/Institution, but should NOT be shared with the Q&amp;I subtask or any other third party due to GDPR considerations.)</p>"},{"location":"elixir_evaluation/#a17-additional-questions-training-contentinformation","title":"A1.7. Additional Questions - Training content/information","text":"<p>These are suggested questions that may be of interest (not compulsory):</p> <p><code>1</code>. What part of the training did you enjoy the most? <code>enjoy</code></p> <p><code>2</code>. What part of the training did you enjoy the least? <code>to_improve</code></p> <p><code>3</code>. The balance of theoretical and practical content was <code>theoretical_practical</code></p> <ul> <li><code>a</code>. Too practical</li> <li><code>b</code>. About right</li> <li><code>c</code>. Too theoretical</li> </ul> <p><code>4</code>. How do you rate the pre-course information given? pre_course_information</p> <ul> <li>Linear scale 1-5</li> <li><code>1</code>. (Very unsatisfactory/Not useful)</li> <li><code>5</code>. Very good/Very useful</li> </ul> <p><code>5</code>. What other topics would you like to see covered in the future? <code>future_topics</code></p> <p><code>6</code>. Any other comments? Comments</p> <p><code>7</code>. PLEASE RATE EACH SESSION OF THE COURSE <code>satisfaction_per_session_YYYY_MM_DD_am/pm</code></p> <ul> <li><code>a</code>. Did not attend</li> <li><code>b</code>. Poor (1)</li> <li><code>c</code>. Satisfactory (2)</li> <li><code>d</code>. Good (3)</li> <li><code>e</code>. Very Good (4)</li> <li><code>f</code>. Excellent (5)</li> </ul> <p><code>8</code>. Comments on teaching staff <code>teaching_staff</code>     Help our teaching staff to improve by providing constructive feedback     Paragraph text answer</p> <p><code>9</code>. Was the course held at a teaching level matching your training? teaching_training_level</p> <p><code>10</code>. STATEMENTS REGARDING WHAT PARTICIPANTS COULD DO before TRAINING (customised to a specific training) skills_before</p> <p><code>11</code>. STATEMENTS REGARDING WHAT PARTICIPANTS CAN DO after TRAINING (customised to a specific training) skills_after</p> <p><code>12</code>. What other topics would you like to see covered in the future? future_topics</p> <p><code>13</code>. Any other comments? Comments_1</p>"},{"location":"elixir_evaluation/#a18-additional-questions-training-logistics","title":"A1.8. Additional Questions - Training logistics","text":"<p>These are suggested questions that may be of interest (not compulsory):</p> <p><code>1</code>. What would be the preferred length of the course? <code>preferred_length</code></p> <ul> <li>Linear scale 1-5 Days</li> </ul> <p><code>2</code>. How did you like the facilities/localities of the course (rooms and surroundings)? <code>course_localities</code></p> <ul> <li>Linear scale 1-5</li> <li><code>1</code>. Not at all</li> <li><code>5</code>. Very much</li> </ul> <p><code>3</code>. How did you like the lunch(es) and \u201cfika(s)\u201d? lunch_fikas</p> <ul> <li>Linear scale 1-5</li> <li><code>1</code>. Not at all</li> <li><code>5</code>. Very much</li> </ul> <p><code>4</code>. Any other comments? <code>Comments_2</code></p> <p>It was a great experience and we are working hard to make it even better. Now go make something great!</p>"},{"location":"introduction/","title":"1. Introduction","text":"<p>On 2025-01-20 the NBIS Training Steering Group had a meeting on course evaluations. The first question was 'How are evaluations evaluated?'.</p> What are the ELIXIR evaluation questions? <p>Read the ELIXIR evaluation questions.</p> <p>This paper deals only with the mandatory questions 5 to and including 9.</p> <p>It is common practice that courses are evaluated by surveys <code>[Brazas &amp; Ouellette, 2016][Gurwitz et al., 2020][Jordan et al., 2018]</code>.</p> <p>Although these surveys are developed with the best intentions, it does not necessarily mean that the questions in such surveys are useful. For example, 2 out of 3 teachers of one NBIS course have the shared verdict that the NBIS questions are -I quote- 'useless', where 1 is neutral and reasoning 'it is what we commonly do'.</p> Source? <p>This story started with one teacher's written-down criticism going through the ELIXIS evaluation, where he uses the term 'useless'.</p> <p>This was followed by the meeting notes where it was decided unanymously against using the survey in unmodified form. In this meeting, only orally, the verdict on this survey was:</p> <ul> <li>2x: useless</li> <li>1x: neutral, as '[such surveys] is what we commonly do'</li> </ul> <p>This disagreement is used as a starting point in evaluating the NBIS course evaluation questions, where the literature is searched for how these questions came to be and by which criteria the best were selected, in the hope of establishing the usefulness of this survey, as well as suggestions for improvements.</p> Which alternatives are suggested? <p>One alternative that is suggested is to use learning outcomes:</p> <ul> <li>An example evaluation using learning outcomes</li> </ul> <p>Another alternative that is suggested is to ask for written feedback only to rate teachers:</p> <ul> <li>An example reflection that used written feedback for its teachers</li> </ul> <p>Also, there is a clear rule to assess the usefulness of an evaluation questions: 'An evaluation question is useful if it is used in a reflection'. Reflections that apply these rules:</p> <ul> <li>Example 1, where some questions are assessed to be useless</li> <li>Example 2, where all questions are used</li> </ul> <p>Besides discussing the current survey question, this paper is the first to give a fully transparent process on how, with the same goals in mind, a similar set of evaluation questions were developed and how the best questions of this set were selected, with the goal of helping to make course evaluations (even) more useful.</p>"},{"location":"introduction/#11-references","title":"1.1. References","text":"<ul> <li><code>[Brazas &amp; Ouellette, 2016]</code>   Brazas, Michelle D., and BF Francis Ouellette.   \"Continuing education workshops in bioinformatics positively impact  research and careers.\" PLoS computational biology 12.6 (2016): e1004916.</li> <li><code>[Gurwitz et al., 2020]</code>   Gurwitz, Kim T., et al.   \"A framework to assess the quality and impact of bioinformatics training   across ELIXIR.\" PLoS computational biology 16.7 (2020): e1007976.   website</li> <li><code>[Jordan et al., 2018]</code>   Jordan, Kari, Fran\u00e7ois Michonneau, and Belinda Weaver.   \"Analysis of Software and Data Carpentry\u2019s pre-and post-workshop surveys.\"   Software Carpentry. Retrieved April 13 (2018): 2023.   PDF</li> </ul>"},{"location":"methods_1/","title":"3.1. Methods 1","text":"Which research question does this answer? <p>This part of the methods is related to RQ1:</p> <p>What is the history of the ELIXIR evaluation questions?</p> <p>A literature search is performed to find out the history of the current ELIXIR evaluation questions, with a focus on answering the following sub-questions:</p> <ul> <li>How were these evaluation questions developed?</li> <li>By which criteria where the best questions selected?</li> </ul> <p>The results can be found at RQ1 results.</p>"},{"location":"methods_2/","title":"3.2. Methods 2","text":"Which research question does this answer? <p>This part of the methods is related to RQ2:</p> <p>How does the academic literature relate to the ELIXIR evaluation questions?</p> <p>A literature search is performed to assess the questions in the ELIXIR SFT.</p> <p>The results can be found at RQ2 results.</p>"},{"location":"methods_3/","title":"3.3. Methods 3","text":"Which research question does this answer? <p>This part of the methods is related to RQ3:</p> <p>Which ELIXIR evaluation questions are concluded from a fully transparent process?</p> <p>To find out which evaluation questions are concluded from a fully transparent process, we use a procedure that involves multiple phases (as shown in figure <code>M3-F1</code>, each having goals as shown in table <code>M3-T1</code></p> <p></p> <p>Figure <code>M3-F1</code>. Overview of the procedure</p> Phase Goal 1 Collect all questions that are considered 'good' by at least 1 NBIS trainer 2 Collect all questions that are considered 'good' by NBIS and ELIXIR 3 Collect all reasons for and against each question 4 Rate all questions 5 Select the questions that are considered good by the NBIS community 6 Merge overlapping questions <p>Table <code>M3-T1</code>: goals of each phase in the procedure</p> <p>Here each step of the procedure is described.</p>"},{"location":"methods_3/#331-phase-1","title":"3.3.1. Phase 1","text":"<p>The goal of phase 1 is to collect all questions that are considered 'good' by at least 1 NBIS trainer.</p> <p>To do so, trainers need to</p> <ul> <li>be aware of this experiment</li> <li>know the goals of ELIXIR</li> <li>be invited to submit their questions</li> <li>do this before a deadline</li> </ul> <p>At an NBIS Training Liaison meeting, introduce this procedure to the people involved in training, as well as advertise in the relevant communication channels. Present, or share an online presentation online that shows the rationale behind this experiment, as well as the goals of ELIXIR.</p> <p>In an online anonymous survey, repeat the rationale of this experiment, as well as the ELIXIR goal of the evaluation.</p> <p>Set a deadline of several weeks. Remind trainers to submit 1 week before the deadline ends.</p> <p>Collect all questions that teachers think are useful anonymously, creating data_set_1_raw.csv.</p> <p>If less than 10 questions are collected, this experiment is cancelled. If more than 10 questions are collected, the authors of this paper are allowed to add their favorite questions too.</p> How does that data set look like? <p>Here is an example:</p> <pre><code>question,reply\nWhat is your favorite animal?,Open question\nWhat is your favorite color?,Open question\nWhat is your favorite colour?,Open question\nWhat is your favorite color?,orange;red\nWhat is your favorite animal?,Open question\n</code></pre> <p>As there may be duplicates in the data set, remove the duplicates transparently, creating data_set_1.csv and describe the process to do so in data_set_1_merge.md.</p> How does that data set look like? <p>Here is an example:</p> <pre><code>question,reply\nWhat is your favorite animal?,Open question\nWhat is your favorite color?,Open question\nWhat is your favorite color?,orange;red\n</code></pre> How does the process description look like? <p>Here is an example:</p> <pre><code># Data set 1 merge\n\nThere were 5 questions.\n\nOf those 5, here are 2 questions with different spelling:\n\n```text\nWhat is your favorite color?,Open question\nWhat is your favorite colour?,Open question\n```\n\nRJCB removed the one with `colour`, resulting in 4 questions.\n\nOf the remaining 4, here are 2 identical questions:\n\n```text\nWhat is your favorite animal?,Open question\nWhat is your favorite animal?,Open question\n```\n\nRJCB removed one of these duplicates, resulting in 3 questions.\n</code></pre>"},{"location":"methods_3/#332-phase-2","title":"3.3.2. Phase 2","text":"<p>Combine <code>Data Set 1</code> with the current NBIS questions. Shuffle these questions randomly, creating data_set_2.csv</p> How does that data set look like? <p>Here is an example:</p> <pre><code>question,reply\nWhat is your favorite color?,orange;red\nWould you recommend the course?,Yes;No;Maybe\nWhat is your favorite animal?,Open question\nWhat is your favorite color?,Open question\n</code></pre>"},{"location":"methods_3/#333-phase-3","title":"3.3.3. Phase 3","text":"<ul> <li>Per question, as the teachers anonymously for reasons why   they would be for or against each question.   The collection of reasonings per questions results in   data_set_3.csv</li> </ul> How does that data set look like? <p>Here is an example:</p> <p></p> <pre><code>question,reply,vote,reason\nWhat is your favorite color?,orange;red,Con,Irrelevant to the course\nWould you recommend the course?,Yes;No;Maybe,Con,This is irrelavant for course quality\nWould you recommend the course?,Yes;No;Maybe,Pro,This is a good proxy for course quality\nWhat is your favorite animal?,Open question,Con,Irrelevant to the course\nWhat is your favorite animal?,Open question,Pro,Would be nice to know\nWhat is your favorite color?,Open question,Con,Irrelevant to the course\n</code></pre>"},{"location":"methods_3/#334-phase-4","title":"3.3.4. Phase 4","text":"<ul> <li>Per question, and its pros and cons, vote anonymously if the question   is useful enough to be included in a survey. Allow 'no', 'yes' and neutral   data_set_4.csv</li> </ul> How does that data set look like? <p>Here is an example:</p> <pre><code>question,reply,vote\nWhat is your favorite color?,orange;red,No\nWhat is your favorite color?,orange;red,No\nWhat is your favorite color?,orange;red,Neutral\nWould you recommend the course?,Yes;No;Maybe,No\nWould you recommend the course?,Yes;No;Maybe,Yes\nWould you recommend the course?,Yes;No;Maybe,Yes\nWhat is your favorite animal?,Open question,No\nWhat is your favorite animal?,Open question,No\nWhat is your favorite animal?,Open question,Yes\nWhat is your favorite color?,Open question,No\nWhat is your favorite color?,Open question,No\nWhat is your favorite color?,Open question,Neutral\n</code></pre>"},{"location":"methods_3/#335-phase-5","title":"3.3.5. Phase 5","text":"<p>From the questions and votes, select the set of questions that had more 'yes' than 'no' votes: these are the questions that this NBIS community thinks are useful.</p> How does that data set look like? <p>From the example data, this would be the result:</p> <pre><code>question,reply,vote\nWould you recommend the course?,Yes;No;Maybe\n</code></pre> <p>The results can be found at data_set_5.csv.</p>"},{"location":"methods_3/#336-phase-6","title":"3.3.6. Phase 6","text":"<p>From the questions that had more 'yes' than 'no' votes, merge potential overlap in questions.</p> <p>The results can be found at data_set_6.csv.</p>"},{"location":"methods_4/","title":"3.4. Methods 4","text":"Which research question does this answer? <p>This part of the methods is related to RQ4:</p> <p>How different are the newly suggested questions from the current ones?</p> <p>The results can be found at RQ4 results.</p>"},{"location":"research_questions/","title":"2. Research questions","text":"<ul> <li>RQ1: What is the history of the ELIXIR evaluation questions?   How were they developed?   By which criteria where the best questions selected?</li> <li>RQ2: How does the academic literature relate to the ELIXIR evaluation   questions?</li> <li>RQ3: Which ELIXIR evaluation questions are concluded from a fully   transparent process?</li> <li>RQ4: How different are the newly suggested questions from the current ones?</li> </ul>"},{"location":"results_1/","title":"4.1. Results of RQ1: What is the history of the ELIXIR evaluation questions?","text":"What are the ELIXIR evaluation questions? <p>Read the ELIXIR evaluation questions.</p> <p>This paper deals only with the mandatory questions 5 to and including 9.</p>"},{"location":"results_1/#411-what-is-the-ancestry-of-the-nbis-questions","title":"4.1.1. What is the ancestry of the NBIS questions?","text":"<p>The paper where these questions were described first in <code>[Gurwitz et al., 2020]</code> . In that paper, one can read that these questions are based on <code>[Jordan et al., 2018]</code> and <code>[Brazas &amp; Ouellette, 2016]</code>. These last two papers do not reference any academic papers on where their questions originated from.</p>"},{"location":"results_1/#412-development-of-the-questions","title":"4.1.2. Development of the questions","text":"<p>ELIXIR developed these evaluation questions to, as quoted from <code>[Gurwitz et al., 2020]</code>:</p> <ul> <li>describe the audience demographic being reached   by ELIXIR training events</li> <li>assess the quality of ELIXIR training events directly   after they have taken place'</li> </ul> <p>The resulting metrics can be found at https://training-metrics-dev.elixir-europe.org/all-reports.</p> <p>This is what is written about how the ELIXIR short-term evaluation questions came to be (quote from <code>[Gurwitz et al., 2020]</code>):</p> <p>We were interested in participant satisfaction as a reflection on training quality in order to be able to inform best practice for ELIXIR training. We acknowledge that training quality is more complex than solely participant satisfaction and that the community would benefit from future work to obtain a fuller picture on training quality.</p> <p>This paragraph shows that this ELIXIR group took the liberty of adding questions besides its two primary sources.</p> <p>Again from <code>[Gurwitz et al., 2020]</code> we read:</p> <p>These metrics were developed out of those already collected by ELIXIR training providers, as well as from discussions with stakeholders, external training providers, and literature review <code>[Brazas &amp; Ouellette, 2016][Jordan et al., 2018]</code></p> <p>There are no references to the literature that was reviewed besides these two papers.</p> <p>Neither does the referred literature:</p> <ul> <li><code>[Brazas &amp; Ouellette, 2016]</code> shows the results of surveys from   bioinformatics workshops. The survey questions where   taken from other sources (i.e., the Society for Experimental Biology   and the Global Organisation for Bioinformatics Learning, Education and   Training), without any reference to the literature.   It is not described how the evaluation questions   came to be and with which reasoning the best were selected</li> <li><code>[Jordan et al., 2018]</code> shows the results of surveys from   Data Carpentry workshops.   Also here, it is not described how the evaluation questions   came to be and with which reasoning the best were selected:   this paper has zero references to the literature</li> </ul> <p>Taking a closer look at the evaluation questions of <code>[Jordan et al., 2018]</code>, we see that some questions of its evaluations were not copied to the ELIXIR evaluation. The reasoning why some questions were copied and some not is unpublished.</p> Which questions were not copied to the ELIXIR evaluation? <p>One such removed evaluation question is to let learners self-assess their confidence in learning outcomes.</p> How does such a question look like? <p>Here is an example of questions to let learners self-assess themselves (from <code>[Plaza et al., 2002]</code>):</p> <p></p> How are the analysing such questions look like? <p>Here we can see the results of learners self-assessing their competences before and after the teaching session, figure from <code>[Jordan et al., 2018]</code>:</p> <p></p> <p>Here we can see a similar results for an earlier paper <code>[Raupach et al., 2011]</code>:</p> <p></p> Should that question have been copied to the ELIXIR evaluation? <p>Probably.</p> <p>We know that this self-assessment does not relate to actual skill <code>[Liaw et al., 2012]</code> (with more studies showing this in that paper). However, there is some evidence that self-assessment is informative to evaluate a course curriculum <code>[Plaza et al., 2002]</code> and teacher effectiveness <code>[Raupach et al., 2011]</code>, although other studies argue that more measurements are needed to properly assess teacher effectiveness <code>[Darling\u2010Hammond et al., 2010]</code>.</p>"},{"location":"results_1/#413-references","title":"4.1.3. References","text":"<ul> <li><code>[Ang et al., 2018]</code> Ang, Lawrence, Yvonne Alexandra Breyer, and Joseph Pitt.   \"Course recommendation as a construct in student evaluations:   will students recommend your course?.\" Studies in Higher Education 43.6   (2018): 944-959.</li> <li><code>[Brazas &amp; Ouellette, 2016]</code>   Brazas, Michelle D., and BF Francis Ouellette.   \"Continuing education workshops in bioinformatics positively impact  research and careers.\" PLoS computational biology 12.6 (2016): e1004916.</li> <li><code>[Darling\u2010Hammond et al., 2010]</code>   Darling\u2010Hammond, Linda, Xiaoxia Newton, and Ruth Chung Wei.   \"Evaluating teacher education outcomes: A study of the Stanford Teacher   Education Programme.\" Journal of education for teaching 36.4 (2010): 369-388.</li> <li><code>[Gurwitz et al., 2020]</code>   Gurwitz, Kim T., et al.   \"A framework to assess the quality and impact of bioinformatics training   across ELIXIR.\" PLoS computational biology 16.7 (2020): e1007976.   website</li> <li><code>[Jordan et al., 2018]</code>   Jordan, Kari, Fran\u00e7ois Michonneau, and Belinda Weaver.   \"Analysis of Software and Data Carpentry\u2019s pre-and post-workshop surveys.\"   Software Carpentry. Retrieved April 13 (2018): 2023.   PDF</li> <li><code>[Liaw et al., 2012]</code>   Liaw, Sok Ying, et al. \"Assessment for simulation learning outcomes: a   comparison of knowledge and self-reported confidence with observed clinical   performance.\" Nurse education today 32.6 (2012): e35-e39.</li> <li><code>[Rox\u00e5 et al., 2021]</code> Rox\u00e5, Torgny, et al.   \"Reconceptualizing student ratings of teaching to support quality discourse   on student learning: a systems perspective.\" Higher Education (2021): 1-21.</li> <li><code>[Raupach et al., 2011]</code>   Raupach, Tobias, et al. \"Towards outcome-based programme evaluation:   using student comparative self-assessments to determine teaching   effectiveness.\" Medical teacher 33.8 (2011): e446-e453.</li> <li><code>[Plaza et al., 2002]</code>   Plaza, Cecilia M., et al.   \"Curricular evaluation using self-efficacy measurements.\"   American Journal of Pharmaceutical Education 66.1 (2002): 51-54.</li> <li><code>[Uttl et al., 2017]</code>   Uttl, Bob, Carmela A. White, and Daniela Wong Gonzalez.   \"Meta-analysis of faculty's teaching effectiveness:   Student evaluation of teaching ratings and student learning are not related.\"   Studies in Educational Evaluation 54 (2017): 22-42.</li> </ul>"},{"location":"results_2/","title":"4.2. Results of RQ2","text":"What are the ELIXIR evaluation questions? <p>Read the ELIXIR evaluation questions.</p> <p>This paper deals only with the mandatory questions 5 to and including 9.</p> <p>With the goal of the SFT ('to improve the course and its materials') in mind, here we go through the mandatory questions that resulted from the process described in the results of Research Question 1. The relevant questions are found in <code>Section 3 - Quality Metrics</code> of the NBIS short-term evaluation. Here, we go through each of these questions in detail.</p>"},{"location":"results_2/#421-question-5","title":"4.2.1. Question 5","text":"<pre><code>5. Have you used the tools/resource(s) covered in the course before?\n\n- Never - Unaware of them\n- Never - Used other service\n- Occasionally\n- Frequently\n</code></pre> <p>Question 5 is an interesting way to evaluate the quality of a course, because it is about something learners have done before the course took place. Searching the literature for 'using previous experience in course evaluations' (and sentences alike) resulted in zero hits.</p> What are the metrics for this question? <p>These are the metrics collected at 2025-01-24 7:04 Stockholm time (https://training-metrics-dev.elixir-europe.org/feedback-report):</p> Reponse n Frequency (%) Never - Unaware of them 4350 23.5 Never - aware of them 3838 20.8 Never - Used other service 1803 9.7 Occasionally 6974 37.7 Frequently 1528 8.3"},{"location":"results_2/#422-question-6","title":"4.2.2. Question 6","text":"<pre><code>6. Will you use the tools/resource(s) covered in the course again?\n\n- Yes\n- No\n- Maybe\n</code></pre> <p>Question 6 is another interesting way to evaluate the quality of a course, because it is about the usefulness of the topic being taught, combined with predicting the future. Searching the literature for 'using self-predicted future use of content in course evaluations' (and sentences alike) resulted in zero hits.</p> What are the metrics for this question? <p>These are the metrics collected at 2025-01-24 7:16 Stockholm time (https://training-metrics-dev.elixir-europe.org/feedback-report):</p> Reponse n Frequency (%) Maybe 2822 15.1 No 105 0.6 Yes 15792 84.4"},{"location":"results_2/#423-question-7","title":"4.2.3. Question 7","text":"<pre><code>7. Would you recommend the course?\n\n- Yes\n- No\n- Maybe\n</code></pre> <p>Question 7 attempt to measure course quality by asking the learner if he/she would recommend the course. This question originates from one of the two evaluations that this ELIXIR evaluation is based on (<code>[Jordan et al., 2018]</code>).</p> <p>Searching the literature for 'using course recommendation in evaluation' (and sentences alike) resulted in one relevant hit. This paper, <code>[Ang et al., 2018]</code>, shows that using this question may indeed be a valid way to asses course quality <code>[Ang et al., 2018]</code>.</p> What are the metrics for this question? <p>These are the metrics collected at 2025-01-24 8:27 Stockholm time (https://training-metrics-dev.elixir-europe.org/feedback-report):</p> Reponse n Frequency (%) Maybe 19597 89.5 No 1790 8.2 Yes 519 2.4"},{"location":"results_2/#424-question-8","title":"4.2.4. Question 8","text":"<pre><code>8. What is your overall rating for the course\n\n- Poor (1)\n- Satisfactory (2)\n- Good (3)\n- Very Good (4)\n- Excellent (5)\n</code></pre> <p>Question 8 too attempts to measure course quality by asking the learner to rate it. This question is absent from the two questionnaires (i.e. those described in <code>[Brazas &amp; Ouellette, 2016]</code> and <code>[Jordan et al., 2018]</code>) this questionnaire is based one.</p> <p>Searching the literature for 'using course satisfaction in evaluations' (and sentences alike) resulted in many relevant papers. The most important paper is a meta-analysis, which concluded that there is no relation between training quality and participant satisfaction <code>[Uttl et al., 2017]</code> and this meta-analysis gives some examples how problematic this metric is.</p> What are the metrics for this question? <p>These are the metrics collected at 2025-01-24 8:28 Stockholm time (https://training-metrics-dev.elixir-europe.org/feedback-report):</p> Reponse n Frequency (%) Excellent 7736 37 Very good 8437 40.4 Good 3543 17 Satisfactory 993 4.8 Poor 192 0.9"},{"location":"results_2/#425-question-9","title":"4.2.5. Question 9","text":"<pre><code>9. A. May we contact you by email in the future for more feedback?\n\n- Yes\n- No\n</code></pre> <p>Question 9 is an interesting way to measure the course quality, based on the learner being willing to answer questions on the future. It seems more likely that question should be placed outside of the section <code>Section 3 - Quality Metrics</code>.</p> <p>Searching the literature for 'using future contact in course evaluation' (and sentences alike) resulted in zero relevant hits.</p> What are the metrics for this question? <p>These are the metrics collected at 2025-01-24 8:32 Stockholm time (https://training-metrics-dev.elixir-europe.org/feedback-report):</p> Reponse n Frequency (%) No 8756 49.7 Yes 8860 50.3"},{"location":"results_2/#426-references","title":"4.2.6. References","text":"<ul> <li><code>[Ang et al., 2018]</code> Ang, Lawrence, Yvonne Alexandra Breyer, and Joseph Pitt.   \"Course recommendation as a construct in student evaluations:   will students recommend your course?.\" Studies in Higher Education 43.6   (2018): 944-959.</li> <li><code>[Brazas &amp; Ouellette, 2016]</code>   Brazas, Michelle D., and BF Francis Ouellette.   \"Continuing education workshops in bioinformatics positively impact  research and careers.\" PLoS computational biology 12.6 (2016): e1004916.</li> <li><code>[Jordan et al., 2018]</code>   Jordan, Kari, Fran\u00e7ois Michonneau, and Belinda Weaver.   \"Analysis of Software and Data Carpentry\u2019s pre-and post-workshop surveys.\"   Software Carpentry. Retrieved April 13 (2018): 2023.   PDF</li> <li>[Uttl et al., 2017]   Uttl, Bob, Carmela A. White, and Daniela Wong Gonzalez.   \"Meta-analysis of faculty's teaching effectiveness:   Student evaluation of teaching ratings and student learning are not related.\"   Studies in Educational Evaluation 54 (2017): 22-42.</li> </ul>"},{"location":"results_3/","title":"4.3. Results of RQ3","text":"Which research question does this answer? <p>This part of the methods is related to RQ3:</p> <p>Which ELIXIR evaluation questions are concluded from a fully transparent process?</p> <p>On 2025-02-17:</p> <ul> <li>a presentation was given at the NBIS TrSG, with 5 people attending.   The presentation lasted around 10 minutes, after which there   was 15 minutes of questions</li> <li>after the meeting, a video of that presentation was recorded</li> </ul> Where can I find the presentations and video? <p>The presentations and videos can be found at presentations.</p> <ul> <li>NBIS teachers were invited to participate</li> </ul> How did the invite on Slack look like? <p>Dear trainers,</p> <p>You are all invited in an experiment to find 'good' course evaluation questions, that have the goal to assess course quality, in a way that is (1) transparent, (2) as a group, and (3) uses informed decision making. This is part of a (draft-stage) paper you can find here.</p> <p>You are invited to share your favorite evaluation question(s) in this form here.</p> <p>If you need more background:</p> <ul> <li>You can find the presentation on this on YouTube here</li> <li>You can view the presentation slides here</li> </ul> <p>You are encouraged to share the link to the form with NBIS trainers: the goal is to find the set of evaluation questions that we think is useful enough to ask to all our course participants.</p> <p>Thanks and cheers, Richel</p>"},{"location":"results_4/","title":"Results 4","text":""},{"location":"results_4/#44-results-of-rq4-how-different-are-the-newly-suggested-questions-from-the-current-ones","title":"4.4. Results of RQ4: How different are the newly suggested questions from the current ones?","text":"<pre><code>No results yes\n</code></pre>"},{"location":"meeting_notes/20250306/","title":"2025-03-06 Meeting Daniel Wibberg","text":"<p>Goal: find out if and to what extent we collaborate on the 'ELIXIR evaluation evaluation' paper</p> <ul> <li>Questions about this draft?</li> <li>Join as author?</li> <li> <p>Join experiment?</p> </li> <li> <p>May I quote you on this, from the Q&amp;A document of last clinic (at <code>[URL]</code>)?   If yes, how:</p> <ul> <li>[x] 'Daniel Wibberg, co-author of [the 2020 paper] and a trainer   using this evaluation (personal communication)'</li> </ul> </li> </ul> <pre><code>Honestly, I haven't found the ELIXIR questions particularly useful\nin improving my course.\nWhile they do serve the purpose of collecting numbers for KPIs,\nthey don't provide the in-depth, qualitative feedback needed\nto make substantial course improvements.\nThe questions focus more on statistical outputs\nrather than offering insights into student experiences\nor areas for pedagogical enhancement.\nAs a result, they fall short in helping to identify specific aspects\nof the course that could benefit from revision or innovation.\n</code></pre> <ul> <li>May I quote you on this, from the Q&amp;A document of last clicic (at <code>[URL]</code>)?   If yes, how:<ul> <li>'Daniel Wibberg, co-author of [the 2020 paper] (personal communication)'</li> <li>'A co-author of [the 2020 paper] (personal communication)'</li> <li>'Someone involved in [the 2020 paper] (personal communication)'</li> <li>'Anonymous source (personal communication)'</li> </ul> </li> </ul> <pre><code>[text]\n</code></pre> <p>To do:</p> <ul> <li>RB: add that we may add responses too, if there are already 10+ responses</li> <li>RB: Make the tone more optimistic</li> <li>DW: Contact Kim Guruwitz</li> <li>DW + RB: Find a survey expert</li> <li>Get more people filling in:<ul> <li>DW: deNBI (20)</li> <li>DW: bioinformatics.de (1000)</li> <li>DW: ELIXIR training platform Slack</li> <li>DW: Training platform meeting end of March ~30</li> <li>DW: TRIANGLE ~10</li> </ul> </li> <li>Title: Improving the ELIXIR evaluation for both management and trainers</li> <li>DW: dwibberg</li> </ul> <p>Schedule:</p> <ul> <li>April 6th: check again, remind if needed</li> <li>May 6th: check results,</li> <li>&lt;10 responses at Sep 1 we abort</li> </ul> <p>Next meeting:</p> <p>Friday April 11th 13:00-(max)14:00, BW will send invite</p>"},{"location":"meeting_notes/20250411/","title":"2025-04-11 Meeting Daniel Wibberg","text":"<p>Goal: Progress</p> <ul> <li>Questions about this draft?</li> <li>Join as author?</li> <li>Join experiment?</li> </ul> <p>To do:</p> <ul> <li>[ ] RB: Add quote</li> <li>[X] RB: add that we may add responses too, if there are already 10+ responses</li> <li>[ ] DW: Contact Kim Guruwitz</li> <li>[ ] DW + RB: Find a survey expert</li> <li>Get more people filling in:<ul> <li>Status: 2 responses</li> <li>DW: deNBI (20)</li> <li>DW: bioinformatics.de (1000)</li> <li>DW: ELIXIR training platform Slack</li> <li>DW: Training platform meeting end of March ~30</li> <li>DW: TRIANGLE ~10</li> </ul> </li> <li>[x] Title: Improving the ELIXIR evaluation for both management and trainers</li> <li>[x] RB: Make the tone more optimistic</li> <li>[x] RB: add <code>dwibberg</code> as collaborator<ul> <li>[ ] DW: accept GitHub invite to be a collaborator</li> </ul> </li> </ul> <p>Schedule:</p> <ul> <li>April 6th: check again, remind if needed: 2 responses</li> <li>May 6th: check results,</li> <li>&lt;10 responses at Sep 1 we abort</li> </ul> <p>Next meeting:</p> <p>Friday April 11th 13:00-(max)14:00, BW will send invite</p>"},{"location":"papers/","title":"Papers","text":"<p>Here one can download the legally redistributable papers used in the research.</p> Reference PDF <code>[Jordan et al., 2018]</code> jordan_et_al_2018.pdf"},{"location":"papers/#references","title":"References","text":"<ul> <li><code>[Jordan et al., 2018]</code>   Jordan, Kari, Fran\u00e7ois Michonneau, and Belinda Weaver.   \"Analysis of Software and Data Carpentry\u2019s pre-and post-workshop surveys.\"   Software Carpentry. Retrieved April 13 (2018): 2023.   PDF</li> </ul>"},{"location":"presentations/","title":"Presentations","text":"<p>Here are the presentations for this paper.</p> Step Downloads 1 step_1.odp, step_1.pdf, YouTube video"}]}